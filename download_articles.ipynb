{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from lxml import html\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def build_search_url(query: str, size: int, start: int) -> str:\n",
    "    query_parsed = '+'.join(query.split())\n",
    "\n",
    "    return f\"https://arxiv.org/search/?query={query_parsed}&searchtype=all&source=header&order=-announced_date_first&size={size}&start={start}\"\n",
    "\n",
    "def download_page(url, save_folder):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        file_name = os.path.join(save_folder, url.split(\"/\")[-1] + \".html\")\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Page downloaded: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download page from: {url}\")\n",
    "\n",
    "def download_articles(articles, save_folder):\n",
    "    for article in articles:\n",
    "        link = article.find(\"a\")['href']\n",
    "        html_link = link.replace(\"arxiv.org\", \"ar5iv.org\")\n",
    "\n",
    "        response_mod = requests.head(html_link, allow_redirects=True)\n",
    "        if response_mod.url == link:\n",
    "            print(f\"{link} redirects to the original link. Skipping...\")\n",
    "        else:\n",
    "            print(f\"{link} doesn't redirect to the original link. Downloading...\")\n",
    "            download_page(html_link, save_folder)\n",
    "\n",
    "def get_articles(query: str, save_folder: str):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "\n",
    "    start = 0\n",
    "    size = 200\n",
    "\n",
    "    while True:\n",
    "        response = requests.get(build_search_url(query, size, start))\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            articles = soup.find_all(\"p\", class_=\"list-title is-inline-block\")\n",
    "\n",
    "            if not articles or len(articles) == 0:\n",
    "                break\n",
    "\n",
    "            download_articles(articles, save_folder)\n",
    "\n",
    "            start += size\n",
    "        else:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/abs/2301.02993 doesn't redirect to the original link. Downloading...\n",
      "Page downloaded: articles/deepmatcher\\2301.02993.html\n",
      "https://arxiv.org/abs/2211.06975 doesn't redirect to the original link. Downloading...\n",
      "Page downloaded: articles/deepmatcher\\2211.06975.html\n",
      "https://arxiv.org/abs/2103.04489 doesn't redirect to the original link. Downloading...\n",
      "Page downloaded: articles/deepmatcher\\2103.04489.html\n",
      "https://arxiv.org/abs/2102.07134 doesn't redirect to the original link. Downloading...\n",
      "Page downloaded: articles/deepmatcher\\2102.07134.html\n",
      "https://arxiv.org/abs/1802.05664 doesn't redirect to the original link. Downloading...\n",
      "Page downloaded: articles/deepmatcher\\1802.05664.html\n",
      "https://arxiv.org/abs/1506.07656 doesn't redirect to the original link. Downloading...\n",
      "Page downloaded: articles/deepmatcher\\1506.07656.html\n",
      "https://arxiv.org/abs/1503.02427 doesn't redirect to the original link. Downloading...\n",
      "Page downloaded: articles/deepmatcher\\1503.02427.html\n"
     ]
    }
   ],
   "source": [
    "search_query = \"deepmatcher\"\n",
    "directory_path = \"articles/deepmatcher\"\n",
    "\n",
    "get_articles(search_query, directory_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def clean_html(element):\n",
    "    for child in element.iter():\n",
    "        # Remove all attributes except rowspan and colspan\n",
    "        attributes_to_keep = {'rowspan', 'colspan'}\n",
    "        for attribute in list(child.attrib.keys()):\n",
    "            if attribute not in attributes_to_keep:\n",
    "                del child.attrib[attribute]\n",
    "        # Remove classes\n",
    "        if 'class' in child.attrib:\n",
    "            del child.attrib['class']\n",
    "        # Remove inline styles\n",
    "        if 'style' in child.attrib:\n",
    "            del child.attrib['style']\n",
    "        # Remove IDs\n",
    "        if 'id' in child.attrib:\n",
    "            del child.attrib['id']\n",
    "    return element\n",
    "\n",
    "\n",
    "def extract_tables_from_html(html_file_path):\n",
    "    tree = html.parse(html_file_path)\n",
    "    tables_xpath = tree.xpath('//figure[contains(@class, \"ltx_table\")]')\n",
    "\n",
    "    extracted_tables = []\n",
    "    for table in tables_xpath:\n",
    "        clean_table = clean_html(table)\n",
    "        table_string = html.tostring(clean_table, method='html', encoding='utf-8').decode(encoding='utf-8')\n",
    "        table_string = table_string.replace('\\n', '')\n",
    "        extracted_tables.append(table_string)\n",
    "\n",
    "    return extracted_tables\n",
    "\n",
    "def extract_tables_from_directory(dir_path):\n",
    "    extracted_tables_map = {}\n",
    "\n",
    "    for filename in os.listdir(dir_path):\n",
    "        if filename.endswith(\".html\"):\n",
    "            file_path = os.path.join(dir_path, filename)\n",
    "            article_id = os.path.splitext(filename)[0]\n",
    "            extracted_tables = extract_tables_from_html(file_path)\n",
    "            extracted_tables_map[article_id] = extracted_tables\n",
    "\n",
    "    return extracted_tables_map\n",
    "\n",
    "def save_tables_to_json(extracted_tables_map, output_file):\n",
    "    filtered_tables_map = {article_id: tables for article_id, tables in extracted_tables_map.items() if tables}\n",
    "    with open(output_file, 'w') as json_file:\n",
    "        json.dump(filtered_tables_map, json_file, indent=4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID: 1503.02427 - # Tables Found: 2\n",
      "Article ID: 1506.07656 - # Tables Found: 7\n",
      "Article ID: 1802.05664 - # Tables Found: 0\n",
      "Article ID: 2102.07134 - # Tables Found: 3\n",
      "Article ID: 2103.04489 - # Tables Found: 9\n",
      "Article ID: 2211.06975 - # Tables Found: 13\n",
      "Article ID: 2301.02993 - # Tables Found: 8\n"
     ]
    }
   ],
   "source": [
    "directory_path = \"articles/deepmatcher\"\n",
    "articles_tables_map = extract_tables_from_directory(directory_path)\n",
    "\n",
    "for article_id, article_tables in articles_tables_map.items():\n",
    "    print(f\"Article ID: {article_id} - # Tables Found: {len(article_tables)}\")\n",
    "\n",
    "extraction_path = 'extracted_tables/deepmatcher.json'\n",
    "\n",
    "save_tables_to_json(articles_tables_map, extraction_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
